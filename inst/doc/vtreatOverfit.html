<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="John Mount, Nina Zumel" />

<meta name="date" content="2018-07-15" />

<title>vtreat overfit</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">vtreat overfit</h1>
<h4 class="author"><em>John Mount, Nina Zumel</em></h4>
<h4 class="date"><em>2018-07-15</em></h4>



<p>Example showing safe “best practice” use of the <a href="https://cran.r-project.org/package=vtreat">‘vtreat’</a> variable preparation library. For more on vtreat see <a href="http://www.win-vector.com/blog/2015/09/vtreat-up-on-cran/">here</a> and <a href="https://github.com/WinVector/vtreat">here</a>.</p>
<p>Below we generate an example data frame with no relation between x and y. We are using a synthetic data set so we know what the “right answer is” (no signal). False fitting on no-signal variables is bad for several reasons:</p>
<ul>
<li>It creates undesirable biases in variable quality estimates and in subsequent models.</li>
<li>It “hides degrees of freedom” from subsequent models.</li>
<li>It creates the false impression you have a good result (which you may fail to falsify).</li>
<li>Complex bad variables can starve out simple weak good variables.</li>
</ul>
<p>This example shows things we don’t want to happen, and then the additional precautions that help prevent them.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">22626</span>)
d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">sample</span>(<span class="kw">paste</span>(<span class="st">'level'</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>,<span class="dt">sep=</span><span class="st">''</span>),<span class="dv">2000</span>,<span class="dt">replace=</span><span class="ot">TRUE</span>)) <span class="co"># independent variable.</span>
d<span class="op">$</span>y &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="kw">nrow</span>(d))<span class="op">&gt;</span><span class="fl">0.5</span>  <span class="co"># the quantity to be predicted, notice: independent of variables.</span>
d<span class="op">$</span>rgroup &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="dv">100</span><span class="op">*</span><span class="kw">runif</span>(<span class="kw">nrow</span>(d)))  <span class="co"># the random group used for splitting the data set, not a variable.</span></code></pre></div>
<div id="bad-practice-using-the-same-data-to-treat-and-to-train" class="section level2">
<h2>Bad Practice: Using the same data to treat and to train</h2>
<p>Using the same set of data to prepare the variable encoding and train the model can lead to the false belief (derived from the training set) that the model fit well. This is largely due to the treated variable appearing to consume only one degree of freedom, when it in fact consumes many more. In many cases a reasonable setting of <code>pruneSig</code> (say 0.01) will help against a noise variable being considered desirable, but selected variables may still be mis-used by downstream modeling.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrain &lt;-<span class="st"> </span>d[d<span class="op">$</span>rgroup<span class="op">&lt;=</span><span class="dv">80</span>,,drop=<span class="ot">FALSE</span>]
dTest &lt;-<span class="st"> </span>d[d<span class="op">$</span>rgroup<span class="op">&gt;</span><span class="dv">80</span>,,drop=<span class="ot">FALSE</span>]
<span class="kw">library</span>(<span class="st">'vtreat'</span>)
treatments &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">designTreatmentsC</span>(dTrain,<span class="st">'x'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>,
  <span class="dt">rareCount=</span><span class="dv">0</span> <span class="co"># Note: usually want rareCount&gt;0, setting to zero to illustrate problem</span>
)</code></pre></div>
<pre><code>## [1] &quot;vtreat 1.2.4 inspecting inputs Sun Jul 15 17:02:45 2018&quot;
## [1] &quot;designing treatments Sun Jul 15 17:02:45 2018&quot;
## [1] &quot; have initial level statistics Sun Jul 15 17:02:45 2018&quot;
## [1] &quot; scoring treatments Sun Jul 15 17:02:45 2018&quot;
## [1] &quot;have treatment plan Sun Jul 15 17:02:45 2018&quot;
## [1] &quot;rescoring complex variables Sun Jul 15 17:02:45 2018&quot;
## [1] &quot;done rescoring complex variables Sun Jul 15 17:02:45 2018&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrainTreated &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatments,dTrain,
  <span class="dt">pruneSig=</span><span class="kw">c</span>() <span class="co"># Note: usually want pruneSig to be a small fraction, setting to null to illustrate problem</span>
)
m1 &lt;-<span class="st"> </span><span class="kw">glm</span>(y<span class="op">~</span>x_catB,<span class="dt">data=</span>dTrainTreated,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">'logit'</span>))
<span class="kw">print</span>(<span class="kw">summary</span>(m1))  <span class="co"># notice low residual deviance</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.79793  -0.75624   0.00611   0.75631   1.79803  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.02692    0.07046   0.382    0.702    
## x_catB       1.00630    0.11333   8.879   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2270.5  on 1637  degrees of freedom
## Residual deviance: 1146.2  on 1636  degrees of freedom
## AIC: 1150.2
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrain<span class="op">$</span>predM1 &lt;-<span class="st"> </span><span class="kw">predict</span>(m1,<span class="dt">newdata=</span>dTrainTreated,<span class="dt">type=</span><span class="st">'response'</span>)

<span class="co"># devtools::install_github(&quot;WinVector/WVPlots&quot;)</span>
<span class="co"># library('WVPlots')</span>
plotRes &lt;-<span class="st"> </span><span class="cf">function</span>(d,predName,yName,title) {
  <span class="kw">print</span>(title)
  tab &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">truth=</span>d[[yName]],<span class="dt">pred=</span>d[[predName]]<span class="op">&gt;</span><span class="fl">0.5</span>)
  <span class="kw">print</span>(tab)
  diag &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">vapply</span>(<span class="kw">seq_len</span>(<span class="kw">min</span>(<span class="kw">dim</span>(tab))),
                     <span class="cf">function</span>(i) tab[i,i],<span class="kw">numeric</span>(<span class="dv">1</span>)))
  acc &lt;-<span class="st"> </span>diag<span class="op">/</span><span class="kw">sum</span>(tab)
<span class="co">#  if(requireNamespace(&quot;WVPlots&quot;,quietly=TRUE)) {</span>
<span class="co">#     print(WVPlots::ROCPlot(d,predName,yName,title))</span>
<span class="co">#  }</span>
  <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">'accuracy'</span>,acc))
}

<span class="co"># evaluate model on training</span>
<span class="kw">plotRes</span>(dTrain,<span class="st">'predM1'</span>,<span class="st">'y'</span>,<span class="st">'model1 on train'</span>)</code></pre></div>
<pre><code>## [1] &quot;model1 on train&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   711   97
##   TRUE    245  585
## [1] &quot;accuracy 0.791208791208791&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># evaluate model on test</span>
dTestTreated &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatments,dTest,<span class="dt">pruneSig=</span><span class="kw">c</span>())
dTest<span class="op">$</span>predM1 &lt;-<span class="st"> </span><span class="kw">predict</span>(m1,<span class="dt">newdata=</span>dTestTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTest,<span class="st">'predM1'</span>,<span class="st">'y'</span>,<span class="st">'model1 on test'</span>)</code></pre></div>
<pre><code>## [1] &quot;model1 on test&quot;
##        pred
## truth   FALSE TRUE
##   FALSE    80   95
##   TRUE     98   89
## [1] &quot;accuracy 0.466850828729282&quot;</code></pre>
<p>The above is bad: we saw a “significant” model fit on training data (even though there is no relation to be found). This means the treated training data can be confusing to machine learning techniques and to the analyst. The issue is that the training data is no longer exchangeable with the test data because the training data was used to build the variable encodings. One way to avoid this is to not use the training data for variable encoding construction, but instead use a third set for this task.</p>
<div id="what-went-wrong" class="section level3">
<h3>What went wrong?</h3>
<p>Notice that vtreat did not think there was any usable signal, and did not want us to use the variables: the values in <code>treatments$scoreFrame$sig</code> are all much larger than a nominally acceptable significance level like 0.05. The variables stayed in our model because we did not prune them (<em>ie</em> we set <code>pruneSig=c()</code>). Also notice we set <code>rareCount=0</code>, which allows the use of very rare levels (which help drive the problem).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(treatments<span class="op">$</span>scoreFrame)</code></pre></div>
<pre><code>##   varName varMoves          rsq       sig needsSplit extraModelDegrees
## 1  x_catP     TRUE 8.752567e-05 0.6557535       TRUE               819
## 2  x_catB     TRUE 2.016246e-05 0.8305800       TRUE               819
##   origName code
## 1        x catP
## 2        x catB</code></pre>
<p>Subsequently, the down-stream machine learning (in this case a standard logistic regression) used the variable incorrectly. The modeling algorithm gave the variable a non-negligible coefficient (around 3) that it thought was reliably bounded away from zero; it also believed that the resulting model almost halved deviance (when in fact it explained nothing). So any variables that do get through may have distributional issues (and misleadingly low apparent degrees of freedom).</p>
<p><strong>Rare levels of a categorical variable</strong></p>
<p>The biggest contributors to this distributional issue tend to be rare levels of categorical variables. Since the individual levels are rare we have unreliable estimates for their effects, and if there are very many of them we may see quite a large effect in aggregate. To help combat this we have a control called <code>rareLevels</code>. Any level that is observed no more than <code>rareLevels</code> times during training is re-mapped to a new special level called <em>rare</em> and not allowed to directly contribute (i.e. can not generate unique indicator columns, and doesn’t have a direct effect on <code>catB</code> or <code>catN</code> encodings). If all the rare levels have a distinct behavior after grouping, the <em>rare</em> level can capture that.</p>
<p><strong>Impact-coding of categorical variables with many levels</strong></p>
<p>Another undesirable effect is over-estimating significance of derived variable fit for <code>catB</code> and <code>catN</code> impact-coded variables. To fight this vtreat attempts to estimate out of sample or cross-validated effect significances (when it has enough data). With enough data, setting the <code>pruneSig</code> parameter during <code>prepare()</code> will help remove noise variables. One can set <code>pruneSig</code> to something like <em>1/number-of-columns</em> to ensure that with high probability only an constant number of truly useless variables make it to later modeling. However, the significance of a given effect size for variables that actually have some signal (i.e. non-noise variables) can still be sensitive to in/out sample scoring and the hiding of degrees of freedom that occurs when a large categorical variable (that represents a large number of degrees of freedom) is re-coded as an impact or effect (which appears to have only a single degree of freedom).</p>
<p>We next show how to avoid these undesirable illusory effects: better practice in partitioning and using training data. We are doing more with our data (essentially chaining models), so we have to take a bit more care with our data.</p>
</div>
</div>
<div id="correct-practice-12-use-different-data-to-treat-and-train" class="section level2">
<h2>Correct Practice 1/2: Use different data to treat and train</h2>
<p>Below is part of our suggested work pattern: coding/train/test split.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dCode &lt;-<span class="st"> </span>d[d<span class="op">$</span>rgroup<span class="op">&lt;=</span><span class="dv">20</span>,,drop=<span class="ot">FALSE</span>]
dTrain &lt;-<span class="st"> </span>d[(d<span class="op">$</span>rgroup<span class="op">&gt;</span><span class="dv">20</span>) <span class="op">&amp;</span><span class="st"> </span>(d<span class="op">$</span>rgroup<span class="op">&lt;=</span><span class="dv">80</span>),,drop=<span class="ot">FALSE</span>]
treatments &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">designTreatmentsC</span>(dCode,<span class="st">'x'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>,
                                        <span class="dt">rareCount=</span><span class="dv">0</span>,  <span class="co"># Note set this to something larger, like 5</span>
                                        <span class="dt">rareSig=</span><span class="kw">c</span>() <span class="co"># Note set this to something like 0.3</span>
)</code></pre></div>
<pre><code>## [1] &quot;vtreat 1.2.4 inspecting inputs Sun Jul 15 17:02:46 2018&quot;
## [1] &quot;designing treatments Sun Jul 15 17:02:46 2018&quot;
## [1] &quot; have initial level statistics Sun Jul 15 17:02:46 2018&quot;
## [1] &quot; scoring treatments Sun Jul 15 17:02:46 2018&quot;
## [1] &quot;have treatment plan Sun Jul 15 17:02:46 2018&quot;
## [1] &quot;rescoring complex variables Sun Jul 15 17:02:46 2018&quot;
## [1] &quot;done rescoring complex variables Sun Jul 15 17:02:46 2018&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrainTreated &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatments,dTrain,
                                 <span class="dt">pruneSig=</span><span class="kw">c</span>() <span class="co"># Note: set this to filter, like 0.05 or 1/nvars</span>
)
m2 &lt;-<span class="st"> </span><span class="kw">glm</span>(y<span class="op">~</span>x_catB,<span class="dt">data=</span>dTrainTreated,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">'logit'</span>))
<span class="kw">print</span>(<span class="kw">summary</span>(m2)) <span class="co"># notice high residual deviance</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.190  -1.181   1.165   1.174   1.183  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)  0.008294   0.057427   0.144    0.885
## x_catB      -0.002186   0.010944  -0.200    0.842
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1681.6  on 1212  degrees of freedom
## Residual deviance: 1681.5  on 1211  degrees of freedom
## AIC: 1685.5
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrain<span class="op">$</span>predM2 &lt;-<span class="st"> </span><span class="kw">predict</span>(m2,<span class="dt">newdata=</span>dTrainTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTrain,<span class="st">'predM2'</span>,<span class="st">'y'</span>,<span class="st">'model2 on train'</span>)</code></pre></div>
<pre><code>## [1] &quot;model2 on train&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   100  504
##   TRUE     97  512
## [1] &quot;accuracy 0.504534212695796&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We do not advise creating dCodeTreated for any purpose other than</span>
<span class="co"># diagnostic plotting.  You should not use the treated coding data</span>
<span class="co"># for anything (as that would undo the benefit of having a separate</span>
<span class="co"># coding data subset).</span>
dCodeTreated &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatments,dCode,<span class="dt">pruneSig=</span><span class="kw">c</span>())
dCode<span class="op">$</span>predM2 &lt;-<span class="st"> </span><span class="kw">predict</span>(m2,<span class="dt">newdata=</span>dCodeTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dCode,<span class="st">'predM2'</span>,<span class="st">'y'</span>,<span class="st">'model2 on coding set'</span>)</code></pre></div>
<pre><code>## [1] &quot;model2 on coding set&quot;
##        pred
## truth   FALSE TRUE
##   FALSE     0  204
##   TRUE    182   39
## [1] &quot;accuracy 0.0917647058823529&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTestTreated &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatments,dTest,<span class="dt">pruneSig=</span><span class="kw">c</span>())
dTest<span class="op">$</span>predM2 &lt;-<span class="st"> </span><span class="kw">predict</span>(m2,<span class="dt">newdata=</span>dTestTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTest,<span class="st">'predM2'</span>,<span class="st">'y'</span>,<span class="st">'model2 on test set'</span>)</code></pre></div>
<pre><code>## [1] &quot;model2 on test set&quot;
##        pred
## truth   FALSE TRUE
##   FALSE    31  144
##   TRUE     20  167
## [1] &quot;accuracy 0.546961325966851&quot;</code></pre>
<p>In the above example we saw training and test performance are similar – and equally poor, as they should be since there is no signal. Though it didn’t happen in this case, note the coding set can (falsely) show high performance. This is the bad behavior we wanted to isolate out of the training set.</p>
<p>Remember, the goal isn’t good performance on training- it is good performance on future data (simulated by test). So doing well on training and bad on test is worse than doing bad on both test and training.</p>
<p>There are, of course, other methods to avoid the bias introduced in using the same data to both treat/encode the variables and to train the model. vtreat incorporates a number of these methods, including smoothing (controlled through <code>smFactor</code>) and pruning of rare levels (controlled through <code>rareSig</code>).</p>
</div>
<div id="correct-practice-22-use-simulated-out-of-sample-methods-cross-methods" class="section level2">
<h2>Correct Practice 2/2: Use simulated out of sample methods (cross methods)</h2>
<p>Another effective technique: cross-constructed training frames can also be accessed by using <code>mkCrossFrameCExperiment</code> or <code>mkCrossFrameNExperiment</code>, which we demonstrate here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrain &lt;-<span class="st"> </span>d[d<span class="op">$</span>rgroup<span class="op">&lt;=</span><span class="dv">80</span>,,drop=<span class="ot">FALSE</span>]
xdat &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">mkCrossFrameCExperiment</span>(dTrain,<span class="st">'x'</span>,<span class="st">'y'</span>,<span class="ot">TRUE</span>,
                                  <span class="dt">rareCount=</span><span class="dv">0</span>,  <span class="co"># Note set this to something larger, like 5</span>
                                  <span class="dt">rareSig=</span><span class="kw">c</span>())</code></pre></div>
<pre><code>## [1] &quot;vtreat 1.2.4 start initial treatment design Sun Jul 15 17:02:46 2018&quot;
## [1] &quot; start cross frame work Sun Jul 15 17:02:46 2018&quot;
## [1] &quot; vtreat::mkCrossFrameCExperiment done Sun Jul 15 17:02:46 2018&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">treatments &lt;-<span class="st"> </span>xdat<span class="op">$</span>treatments
<span class="kw">print</span>(treatments<span class="op">$</span>scoreFrame)</code></pre></div>
<pre><code>##   varName varMoves          rsq       sig needsSplit extraModelDegrees
## 1  x_catP     TRUE 1.111652e-04 0.6153933       TRUE               819
## 2  x_catB     TRUE 2.052927e-05 0.8290694       TRUE               819
##   origName code
## 1        x catP
## 2        x catB</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrainTreated &lt;-<span class="st"> </span>xdat<span class="op">$</span>crossFrame
m3 &lt;-<span class="st"> </span><span class="kw">glm</span>(y<span class="op">~</span>x_catB,<span class="dt">data=</span>dTrainTreated,<span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">'logit'</span>))
<span class="kw">print</span>(<span class="kw">summary</span>(m3)) <span class="co"># notice high residual deviance</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x_catB, family = binomial(link = &quot;logit&quot;), 
##     data = dTrainTreated)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.254  -1.189   1.103   1.166   1.233  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) 0.026160   0.049480   0.529    0.597  
## x_catB      0.014767   0.007567   1.952    0.051 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2270.5  on 1637  degrees of freedom
## Residual deviance: 2266.6  on 1636  degrees of freedom
## AIC: 2270.6
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTrainTreated<span class="op">$</span>predM3 &lt;-<span class="st"> </span><span class="kw">predict</span>(m3,<span class="dt">newdata=</span>dTrainTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTrainTreated,<span class="st">'predM3'</span>,<span class="st">'y'</span>,<span class="st">'model3 on train'</span>)</code></pre></div>
<pre><code>## [1] &quot;model3 on train&quot;
##        pred
## truth   FALSE TRUE
##   FALSE   209  599
##   TRUE    182  648
## [1] &quot;accuracy 0.523199023199023&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dTestTreated &lt;-<span class="st"> </span>vtreat<span class="op">::</span><span class="kw">prepare</span>(treatments,dTest,<span class="dt">pruneSig=</span><span class="kw">c</span>())
dTest<span class="op">$</span>predM3 &lt;-<span class="st"> </span><span class="kw">predict</span>(m3,<span class="dt">newdata=</span>dTestTreated,<span class="dt">type=</span><span class="st">'response'</span>)
<span class="kw">plotRes</span>(dTest,<span class="st">'predM3'</span>,<span class="st">'y'</span>,<span class="st">'model3 on test set'</span>)</code></pre></div>
<pre><code>## [1] &quot;model3 on test set&quot;
##        pred
## truth   FALSE TRUE
##   FALSE    48  127
##   TRUE     55  132
## [1] &quot;accuracy 0.497237569060773&quot;</code></pre>
<p>Notice the glm significance is off, but the model quality is similar on train and test, and the scoreFrame significance is a correct indication.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
